---
title: "Word Embeddings"
author: "Zachary Blanks"
output: 
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

Before we begin, let's restart R one more time so that we have a fresh environment for this final module. We are also going to use ggplot and purrr for this section. Phil will go into much greater depth with the purrr module, but I will introduce it today to show some basic functionality.

```{r, warning=FALSE, message=F}
library(keras)
library(tidyverse)
```

# Overview
For our last project we are going to use neural networks, vector embeddings, and natural language processing (NLP) to perform sentiment analysis on an example data set provided by IMDB on $25,000$ movie reviews. Fortunately, this data, similar to MNIST, is a popular one and is available via Keras to introduce this concept. We are not going to achieve anywhere near state of the art performance, but we will use ideas that we've seen before and combine them with new NLP concepts to again show that neural networks can be used to tackle a wide range of unstructured data problems. 

Unsurprisingly, we do have some data prepreprocessing to perform to ensure that our data is ready to run it in a sentiment analysis model. We will also have to do some basic data exploration to get an idea of how large we want our vocabulary to be for the embeddings and how long we want our reviews. 

# Data Exploration
Before we begin with our model, we should perform some simple exploratory analysis to get an idea about our data. One note before we begin -- the data has already been pretty extensively pre-preprocessed. The reviews are not in their raw form, but rather have been converted into natural numbers which correspond to their frequency in the data. For example, if an integer is three, this means that this corresponded to the third most common word.

```{r}
imdb <- dataset_imdb()
```

Now let's split our data into a training and test set
```{r}
X_train <- imdb$train$x
y_train <- imdb$train$y
X_test <- imdb$test$x
y_test <- imdb$test$y
```

To start, let's see what a sample in our training set looks like.

```{r}
X_train[[1]]
```

It seems that a sample is a numeric vector where each entry corresponds to the mapping for that particular word. For example, the second word in the review is the $14^{th}$ most common in the data.

Typically, natural language processing problems are very skewed -- namely, a small number of words cover most of the uses in the data. If this is true, then this typically implies we can shrink the vocabulary without paying a big price in terms of model performance while significantly speeding up computation time (similar to PCA). To check this hypothesis, our first exercise for this project will be to look at the distribution of words in the data. Specifically, I would like you to create a histogram displaying the word usage distribution in each of the reviews. To do this, you will need to represent the training data as a DataFrame and use this DataFrame to make a histogram.

```{r}
create_word_df = function(x, sample_num) {
  # Get the number of times repeat the sample number
  sample_num_vect = rep(sample_num, length(x))
  
  # Return a DataFrame with two columns: the sample_num_vect and the words
  return(tibble(sample_num = sample_num_vect, word = x))
}

# Apply the user-created function to each element of the X_train list
# hence we will need purrr's map
word_df = map2_df(.x = X_train, .y = 1:length(X_train), 
                  .f = ~ create_word_df(.x, .y))

# Using the DataFrame we just created to plot the distribution of words in 
# the data
word_df %>% 
  ggplot(aes(x = word)) + 
  geom_histogram(bins = 30) + 
  labs(x = "Word", y = "Count", title = "Word Distribution")
```

As we expected, a significant majority of the words used in the reviews can be described by roughly the first $5000$ most popular words. Consequently before we train our model, we can reduce our vocabulary to include only those words.

Another element we need to determine before employing our sentiment analysis model is the distribution of review lengths. Our model will expect each review (which we will represent as a vector) to have the same length. Therefore we will need to determine a cut-off point for reviews so that if they are longer than this value we can fix this issue or if they're shorter than this value that we can pad the review with junk. Fortunately the way we created our word DataFrame is "tidy". This will allow us to easily answer this question. Therefore for the second exercise of this section, I want you to generate a boxplot displaying the distribution review lengths.

```{r}
word_df %>% 
  group_by(sample_num) %>% 
  summarize(review_len = n()) %>% 
  ggplot(aes(x = "", y = review_len)) + 
  geom_boxplot() + 
  labs(x = "", y = "Review Length")
```

Looking at the box plot, it looks like a significant majority of the reviews are less than $500$ words. Putting the word frequency and the review length pieces of information together, we're going to grab our data again, this time limiting ourselves to a vocabulary of $5000$ words and the max length of a review is $500$. If a particular review is less than $500$ words, we will just pad it with an appropriate number of zeroes. Because this data is so common, Keras has already provided these functions for us, but if you were doing this in real life, these particular functions would not be too difficult to do yourself.

```{r}
# Grab our data again with the constraints described above
imdb <- dataset_imdb(num_words = 5000, maxlen = 500, seed = 17)
X_train <- imdb$train$x
y_train <- imdb$train$y
X_test <- imdb$test$x
y_test <- imdb$test$y

# Pad our sequences if they're less than 500
X_train <- pad_sequences(sequences = X_train, maxlen = 500)
X_test <- pad_sequences(sequences = X_test, maxlen = 500)
```

Let's take a look at our data to make sure that everything looks good before we proceed.
```{r}
dim(X_train)
```

```{r}
X_train[1, ]
```

Everything looks good! Since our data is in the correct form, we should be ready to implement an embedding and fully connected layer to perform sentiment analysis.

# Sentiment Analysis
As you might imagine from our previous exercises, implementing complex models can be done in a matter of a few lines of codes using Keras. The only new layer we will need for this model is *layer_embedding* which will represent each word as a p-dimensional vector. We will also need a *layer_global_average_pooling_1d* to flatten our word emeddings into a two-dimensional space needed for the fully-connected layers. For our next exercise, I want you to make a word embedding network with the correct vocabulary dimesionality and where the words are represented as $32$-dimesional vectors. Specifically I want you to create a network with an embedding layer, a global average pooling layer, a fully-connected layer with 64 nodes, and then a output layer which can predict binary targets. For binary outputs, you will need to use the "sigmoid" activation function.

```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 5000, output_dim = 32,
                  input_length = 500) %>% 
  layer_global_average_pooling_1d() %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Now that we have defined the model, let's compile it using SGD and then train it.

```{r, warning=F}
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_sgd(),
  metrics = c('accuracy')
)

res = model %>% fit(
  x = X_train, y = y_train,
  epochs = 5, batch_size = 128, 
  validation_split = 0.25
)
```

```{r}
model %>% evaluate(X_test, y_test)
```

Huh. It looks like the model did not do very well. It looks like the model is improving, but it's doing it very slowly. Why could that be?

# Optimizers
Up to this point, we have just used vanilla SGD to as the solution method for our model, but in fact there are a huge number algorithms out there that minimize error for neural networks. For this lecture, we will focus on four: SGD, SGD with momentum, RMSProp, and Adam. If you are interested, there are full papers that provide more details on each of these algorithms.

To see these algorithms in action, let's again break up into groups and have each third of the room apply SGD with momentum, RMSProp, and Adam. Use the same model we had above, but with the respective optimization algorithm. For SGD with momentum, set the momentum parameter equal to $0.9$ and for the other algorithms, use the standard settings. Report the results.

```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 5000, output_dim = 32,
                  input_length = 500) %>% 
  layer_global_average_pooling_1d() %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

# SGD with momentum
model1 = clone_model(model)

model1 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_sgd(momentum = 0.9),
  metrics = c('accuracy')
)

res1 = model1 %>% fit(
  x = X_train, y = y_train,
  epochs = 5, batch_size = 128, 
  validation_split = 0.25, 
  verbose = 0
)

# RMSProp
model2 = clone_model(model)

model2 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

res2 = model2 %>% fit(
  x = X_train, y = y_train,
  epochs = 5, batch_size = 128, 
  validation_split = 0.25, 
  verbose = 0
)

# Adam
model3 = clone_model(model)

model3 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

res3 = model3 %>% fit(
  x = X_train, y = y_train,
  epochs = 5, batch_size = 128, 
  validation_split = 0.25, 
  verbose = 0
)
```

Let's see how each of the model's do

```{r}
plot(res1)
```

```{r}
plot(res2)
```

```{r}
plot(res3)
```

```{r}
model1 %>% evaluate(X_test, y_test)
model2 %>% evaluate(X_test, y_test)
model3 %>% evaluate(X_test, y_test)
```

It seems that the optimization algorithm can make a big difference in terms of performance. Typically RMSProp or Adam are good choices, but like almost everything with machine learning and especially with neural networks, try everything out and see what works best. 

# Vector Representation

In our previous model, we just arbitrarily chose the words to be represented
by 32-dimensional vectors; let's see how sensitive our model is to that
choice; using either a 4, 128, or 256 dimensional vector with the Adam 
optimizer, determine how sensitive the model is to this hyper-parameter

```{r}

# Model with 4 dimensional vector
model1 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 5000, output_dim = 4,
                  input_length = 500) %>% 
  layer_global_average_pooling_1d() %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model1 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

res1 = model1 %>% fit(
  x = X_train, y = y_train,
  epochs = 5, batch_size = 128, 
  validation_split = 0.25, 
  verbose = 0
)

# Model with 128-dimensional vector
model2 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 5000, output_dim = 128,
                  input_length = 500) %>% 
  layer_global_average_pooling_1d() %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model2 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

res2 = model2 %>% fit(
  x = X_train, y = y_train,
  epochs = 5, batch_size = 128, 
  validation_split = 0.25, 
  verbose = 1
)

# Model with 256-dimesional vector
model3 = keras_model_sequential() %>% 
  layer_embedding(input_dim = 5000, output_dim = 256,
                  input_length = 500) %>% 
  layer_global_average_pooling_1d() %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model3 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

res3 = model3 %>% fit(
  x = X_train, y = y_train,
  epochs = 5, batch_size = 128, 
  validation_split = 0.25, 
  verbose = 1
)
```

Evaluate their performance out of sample

```{r}
model1 %>% evaluate(X_test, y_test)
model2 %>% evaluate(X_test, y_test)
model3 %>% evaluate(X_test, y_test)
```

I've now taught you everything you need to start using these techniques in practice. I would now like to switch gears to our Kaggle competition so that you have a chance to apply the techniques to a real dataset and compete against your classmates. 
